{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emilevillette/Emilevillette/blob/main/SIMT_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINFO2241 : Practical Session 8\n",
        "\n",
        "## Exercice 2 : SIMT"
      ],
      "metadata": {
        "id": "J9NMohO5ohjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1 : Converting a simple program into CUDA"
      ],
      "metadata": {
        "id": "yrSLWtIMqV1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, install dependencies"
      ],
      "metadata": {
        "id": "mlhsVQB9qwR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uFu4kmfG2Qc",
        "outputId": "defea3a5-e13e-41d0-99c8-6eb0c22df06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+http://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning http://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-haub_x2t\n",
            "  Running command git clone --filter=blob:none --quiet http://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-haub_x2t\n",
            "  warning: redirecting to https://github.com/andreinechaev/nvcc4jupyter.git/\n",
            "  Resolved http://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4294 sha256=0066a6147b868ad0c023e0780bb9f0359d28f76eedd917afb5d10fa3d16f27c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xc1t8liz/wheels/c8/7e/06/e62d8d9c02dd325871935b2afda42a4784814746d313cf308d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!pip install git+http://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, remember this program you used in the previous exercise :\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "long N = 512;\n",
        "\n",
        "// Naive implementation\n",
        "void VecAdd(float* A, float* B, float* C)\n",
        "{\n",
        "    for (int i = 0; i < N; i++)\n",
        "        C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main()\n",
        "{\n",
        "\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate input vectors h_A and h_B in host memory\n",
        "    float* A = (float*)malloc(size);\n",
        "    float* B = (float*)malloc(size);\n",
        "    float* C = (float*)malloc(size);\n",
        "\n",
        "    // Initialize input vectors\n",
        "    for (int i = 0; i < N; i ++) {\n",
        "        A[i] = (float)i;\n",
        "        B[i] = (float)i;\n",
        "        //C[i] = 0; will be overwritten, don't care\n",
        "    }\n",
        "\n",
        "    // Start computation\n",
        "    printf(\"Launching computation...\\n\");\n",
        "    unsigned long long start = rdtscl();\n",
        "    VecAdd(A, B, C, N);\n",
        "    printf(\"Finished in %llu cycles !\\n\", rdtscl() - start);\n",
        "\n",
        "    printf(\"First floats of C : %f %f %f ...\\n\", C[0], C[1] , C[2]);\n",
        "\n",
        "    // Free host memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    printf(\"Exiting...\");\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "TRN2dbKTq2Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your job is to convert it step by step into a GPU compatible program. Follow each steps carefully. You can use the last box to write your findings step-by-step."
      ],
      "metadata": {
        "id": "FjCdXhstryhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1 : Allocating memory on the device\n",
        "\n",
        "The first step in translating this function into cuda is to allocated memory on the device (i.e. The GPU). There are special functions for that.\n",
        "\n",
        "[Hint](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356)\n",
        "\n",
        "```c\n",
        "// Allocate input vectors h_A and h_B in host memory\n",
        "float* cpu_A = (float*)malloc(size);\n",
        "float* cpu_B = (float*)malloc(size);\n",
        "float* cpu_C = (float*)malloc(size);\n",
        "\n",
        "// Initialize input vectors\n",
        "for (int i = 0; i < N; i ++) {\n",
        "    cpu_A[i] = (float)i;\n",
        "    cpu_B[i] = (float)i;\n",
        "    //C[i] = 0; will be overwritten, don't care\n",
        "}\n",
        "\n",
        "// Now, allocate 3 arrays of float, equivalent of cpu_A,cpu_B and cpu_C, but on the GPU\n",
        "float* gpu_A;\n",
        "float* gpu_B;\n",
        "float* gpu_C;\n",
        "...\n",
        "\n",
        "// Start computation\n",
        "printf(\"Launching computation...\\n\");\n",
        "```"
      ],
      "metadata": {
        "id": "0gkXNswO58Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-wxzxjdurS9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2 : Copying memory from the CPU to the device\n",
        "\n",
        "\n",
        "Now that you allocated your memory on the GPU, you can copy arrays A and B into GPU memory ! There is a simple function to perform this copy, you just have to call it twice.\n",
        "\n",
        "[Hint](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8)\n",
        "\n",
        "```c\n",
        "// Allocate input vectors h_A and h_B in host memory\n",
        "float* cpu_A = (float*)malloc(size);\n",
        "float* cpu_B = (float*)malloc(size);\n",
        "float* cpu_C = (float*)malloc(size);\n",
        "\n",
        "// Initialize input vectors\n",
        "for (int i = 0; i < N; i ++) {\n",
        "    cpu_A[i] = (float)i;\n",
        "    cpu_B[i] = (float)i;\n",
        "    //C[i] = 0; will be overwritten, don't care\n",
        "}\n",
        "\n",
        "// Code from previous Exercice : allocate 3 arrays of float\n",
        "float* gpu_A;\n",
        "float* gpu_B;\n",
        "float* gpu_C;\n",
        "...\n",
        "\n",
        "// Exercice 2 : Copy cpu_A and cpu_B into gpu_A and gpu_B\n",
        "...\n",
        "\n",
        "// Start computation\n",
        "printf(\"Launching computation...\\n\");\n",
        "```"
      ],
      "metadata": {
        "id": "39oci12f8Cny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interlude: Manageing threads in GPUs\n",
        "\n",
        "Before going further, it is important to understand how threads are handled in GPUs.\n",
        "\n",
        "A CUDA thread takes care of a single element of data. As seen in the course, the idea of SIMT is to execute many many threads in parallel. Threfore threads are organized into blocks of 32 threads. And the whole computation to be done, is an ensemble of \"blocks\" to be executed.\n",
        "\n",
        "![GPU blocks](https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/kernel-execution-on-gpu-1-625x438.png)\n",
        "\n",
        "A thread block will be executed by a CUDA Streaming Multiprocessor (SM). It is like the SIMT design seen in the lecture : a single instruction pointer is shared among all threads, advancing on all their independent data element.\n",
        "\n",
        "\n",
        "Let's say you have an array of 2048 elements. You will have to divide it in blocks of a multiple of 32 threads (let's say 32 for now). You will therefore have 64 blocks of 32 threads. The details will be seen in the course, but the NVIDIA GigaThread will assign each block to the CUDA SM. It is a hardware piece, this is not done in software by you or the driver.\n",
        "\n",
        "In our case, a thread does not loop. With CUDA, you just launch many thread that works individually in a scalar way on a single element. But which scalar? In your array of 2048 elements, how to know which thread should access `array[0]` or `array[258]`, if all threads execute the same code ?\n",
        "\n",
        "By using the thread index !\n",
        "\n",
        "CUDA exposes a few global objects. blockDim, blockIdx and threadIdx. Those object will actually have different values for each different threads.\n",
        "In this example we will only use a single dimension, so we will limit ourselves to \".x\" of each element.\n",
        "\n",
        " * blockDim.x <-- the wideness of the block (always 32 as we use 32 threads per blocks)\n",
        " * blockIdx.x <-- the index of the block (from 0 to 63, as we have 64 blocks in total)\n",
        " * threadIdx.x <-- the index of the thread (from 0 to 31, as there is 32 threads per blocks)\n",
        "\n",
        "In practice, one can change the size of the blocks. You will try that later.\n",
        "\n",
        "So, how to compute, in an array of 2048 a unique index for all threads, so that at the end, by launching all 64 blocks of 32 threads, all 2048 elements have been computed ?\n",
        "\n",
        "`int i = ...`"
      ],
      "metadata": {
        "id": "Hv1UFyoewkUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3 : Adapting the function\n",
        "\n",
        "\n",
        "The objective of this function is to run solely on the GPU, all you have is the 3 vectors you initialized and the value `N`, the maximal size of the array. As this function starts with `__global__`, it will run on the GPU. Now, you must take advantage of GPU architecture.\n",
        "\n",
        "Following the discussion in the last block of text, you can compute on which index of A, B and C this thread should operate.\n",
        "\n",
        "One last note : you must verify that the computed index is not bigger than N ! Indeed, if N is not a multiple of 32, then the last threads in the block would work out of bound !\n",
        "\n",
        "\n",
        "```c\n",
        "/**\n",
        "* @param gpu_A : Array A allocated on the GPU\n",
        "* @param gpu_B : Array B allocated on the GPU\n",
        "* @param gpu_C : Array C allocated on the GPU\n",
        "* @param N : Length of each array\n",
        "*/\n",
        "__global__ void VecAdd(float* gpu_A, float* gpu_B, float* gpu_C, long N)\n",
        "{\n",
        "  // Your code here\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "hhsfj9A_-FAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4 : Changing the function call\n",
        "\n",
        "\n",
        "Functions that runs on GPU aren't called normally. We give you this part of the code, but you'll have to tweak parameters such as `threadsPerBlock` and `blocksPerGrid`.\n",
        "\n",
        "\n",
        "```c\n",
        "    printf(\"Launching kernel...\\n\");\n",
        "    // Invoke GPU function\n",
        "    unsigned long long start = rdtscl();\n",
        "    int threadsPerBlock = 128; //Always a multiple of 32 !\n",
        "    int blocksPerGrid =\n",
        "            (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(gpu_A, gpu_B, gpu_C, N);\n",
        "\n",
        "    printf(\"Finished in %llu cycles !\\n\", rdtscl() - start);\n",
        "```"
      ],
      "metadata": {
        "id": "KraNokWTA52W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5 : Copying result from memory to the CPU\n",
        "\n",
        "\n",
        "Now that you called you GPU function and it successfully stored the result of its computations within `gpu_C`, you simply have to copy its content into CPU memory (The opposite of what you did at step 2)\n",
        "\n",
        "\n",
        "```c\n",
        "    printf(\"Launching kernel...\\n\");\n",
        "    // Invoke GPU function\n",
        "    unsigned long long start = rdtscl();\n",
        "    int threadsPerBlock = 128;\n",
        "    int blocksPerGrid =\n",
        "            (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(gpu_A, gpu_B, gpu_C, N);\n",
        "\n",
        "    // Copying gpu_C into cpu_C\n",
        "    ...\n",
        "\n",
        "    \n",
        "    printf(\"Finished in %llu cycles !\\n\", rdtscl() - start);\n",
        "```"
      ],
      "metadata": {
        "id": "KIst0rb0CICI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6 : Freeing memory\n",
        "\n",
        "In this final step, as every respectful developer, you must free your memory.\n",
        "\n",
        "```c\n",
        "    printf(\"First floats of C : %f %f %f ...\\n\", h_C[0], h_C[1] , h_C[2]);\n",
        "    \n",
        "    // Free device memory\n",
        "    cudaFree(gpu_A);\n",
        "    cudaFree(gpu_B);\n",
        "    cudaFree(gpu_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(cpu_A);\n",
        "    free(cpu_B);\n",
        "    free(cpu_C);\n",
        "\n",
        "    printf(\"Exiting...\");\n",
        "```"
      ],
      "metadata": {
        "id": "gVv-mb6eCuye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final code\n",
        "\n",
        "This cell will contain your final code, we recommand that you progressively complete it by following each step described above :"
      ],
      "metadata": {
        "id": "l0krFGG98uhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "\n",
        "unsigned long long rdtscl(void)\n",
        "{\n",
        "    unsigned int lo, hi;\n",
        "    __asm__ __volatile__ (\"rdtsc\" : \"=a\"(lo), \"=d\"(hi));\n",
        "    return ( (unsigned long long)lo)|( ((unsigned long long)hi)<<32 );\n",
        "}\n",
        "\n",
        "// Device code\n",
        "__global__ void VecAdd(float* gpu_A, float* gpu_B, float* gpu_C, int N)\n",
        "{\n",
        "    // CUDA version of the function\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {gpu_C[i] = gpu_A[i] + gpu_B[i];}\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main()\n",
        "{\n",
        "    uint64_t N = 512;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate input vectors h_A and h_B in host memory\n",
        "    float* cpu_A = (float*)malloc(size);\n",
        "    float* cpu_B = (float*)malloc(size);\n",
        "    float* cpu_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialize input vectors\n",
        "    for (int i = 0; i < N; i ++) {\n",
        "        cpu_A[i] = (float)i;\n",
        "        cpu_B[i] = (float)i;\n",
        "        //h_C[i] = 0; will be overwritten, don't care\n",
        "    }\n",
        "\n",
        "    // Allocate vectors in device memory\n",
        "    float* gpu_A;\n",
        "    float* gpu_B;\n",
        "    float* gpu_C;\n",
        "\n",
        "    cudaMalloc( &gpu_A, size );\n",
        "    cudaMalloc( &gpu_B, size );\n",
        "    cudaMalloc( &gpu_C, size );\n",
        "\n",
        "    // Copy vectors from host memory to device memory\n",
        "    cudaMemcpy( gpu_A,cpu_A, size, cudaMemcpyDefault);\n",
        "    cudaMemcpy( gpu_B,cpu_B, size, cudaMemcpyDefault);\n",
        "    cudaMemcpy( gpu_C, cpu_C, size, cudaMemcpyDefault);\n",
        "\n",
        "\n",
        "    printf(\"Launching kernel...\\n\");\n",
        "    // Invoke GPU function\n",
        "    unsigned long long start = rdtscl();\n",
        "    int threadsPerBlock = 128;\n",
        "    int blocksPerGrid =\n",
        "            (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(gpu_A, gpu_B, gpu_C, N);\n",
        "\n",
        "    // Copy result from GPU to CPU\n",
        "    cudaMemcpy(cpu_C, gpu_C, size, cudaMemcpyDefault);\n",
        "\n",
        "\n",
        "    printf(\"Finished in %llu cycles !\\n\", rdtscl() - start);\n",
        "\n",
        "    printf(\"First floats of C : %f %f %f ...\\n\", cpu_C[0], cpu_C[1] , cpu_C[2]);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(gpu_A);\n",
        "    cudaFree(gpu_B);\n",
        "    cudaFree(gpu_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(cpu_A);\n",
        "    free(cpu_B);\n",
        "    free(cpu_C);\n",
        "\n",
        "    printf(\"Exiting...\");\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvdUx1ZhD98l",
        "outputId": "b1347177-0853-40dd-b5ae-c2218bae9a1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o cuda cuda.cu"
      ],
      "metadata": {
        "id": "5GdLvKLiJHkE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda || echo \"Crashed with code : $?\""
      ],
      "metadata": {
        "id": "yU6aoCaUJQTV",
        "outputId": "57536207-eddc-4bd6-bb62-5967415957bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching kernel...\n",
            "Finished in 84188 cycles !\n",
            "First floats of C : 0.000000 2.000000 4.000000 ...\n",
            "Exiting..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2 : Performance\n",
        "\n",
        "Compare the CUDA implementation and the normal one. Can you see a difference? Can you tweak CUDA parameters to change performance?"
      ],
      "metadata": {
        "id": "PZXuQyP0FSC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile naive.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "unsigned long long rdtscl(void)\n",
        "{\n",
        "    unsigned int lo, hi;\n",
        "    __asm__ __volatile__ (\"rdtsc\" : \"=a\"(lo), \"=d\"(hi));\n",
        "    return ( (unsigned long long)lo)|( ((unsigned long long)hi)<<32 );\n",
        "}\n",
        "\n",
        "// Naive implementation\n",
        "void VecAdd(float* A, float* B, float* C, long N)\n",
        "{\n",
        "    for (int i = 0; i < N; i++)\n",
        "        C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main()\n",
        "{\n",
        "\n",
        "    long N = 512;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate input vectors h_A and h_B in host memory\n",
        "    float* A = (float*)malloc(size);\n",
        "    float* B = (float*)malloc(size);\n",
        "    float* C = (float*)malloc(size);\n",
        "\n",
        "    // Initialize input vectors\n",
        "    for (int i = 0; i < N; i ++) {\n",
        "        A[i] = (float)i;\n",
        "        B[i] = (float)i;\n",
        "        //C[i] = 0; will be overwritten, don't care\n",
        "    }\n",
        "\n",
        "    // Start computation\n",
        "\n",
        "    printf(\"Launching computation...\\n\");\n",
        "    unsigned long long start = rdtscl();\n",
        "    VecAdd(A, B, C, N);\n",
        "\n",
        "    printf(\"Finished in %llu cycles !\\n\", rdtscl() - start);\n",
        "\n",
        "    printf(\"First floats of C : %f %f %f ...\\n\", C[0], C[1] , C[2]);\n",
        "\n",
        "    // Free host memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    printf(\"Exiting...\");\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zals_uoGXmn",
        "outputId": "aec35f0f-fbf7-4bc3-8828-be0c6d37f9ba"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing naive.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc -march=native naive.c -o naive"
      ],
      "metadata": {
        "id": "nK9JWVQYGkP0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./naive\n",
        "!./cuda"
      ],
      "metadata": {
        "id": "oQpWAcVHG-zq",
        "outputId": "c233c02a-3706-4d92-ef59-6bef93d9edff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching computation...\n",
            "Finished in 4882 cycles !\n",
            "First floats of C : 0.000000 2.000000 4.000000 ...\n",
            "Exiting...Launching kernel...\n",
            "Finished in 82122 cycles !\n",
            "First floats of C : 0.000000 2.000000 4.000000 ...\n",
            "Exiting..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you conclude on the performance of your CUDA version? Why? Also try to change the threadsPerBlock to 1, 4 or 32. What do you observe? Why?"
      ],
      "metadata": {
        "id": "SBUGWuPL0hC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going further : Matrix multiplication\n",
        "\n",
        "In the previous exercise we worked with a one-dimensional vector. Let's now consider square matrix, a vector with two dimension. CUDA has support for blocks of 2 and 3 dimensions.\n",
        "\n",
        "For a matrix multiplication, composed of 3 loops, the idea is to have each thread do the internal loop using the naive matrix multiplication. Indeed, the optimized version we saw requires complex synchronization to add results from different threads. This is beyond this course.\n",
        "\n",
        "Consider a matrix with dimension `N=128`. We are limited by the size of blocks. It would be easy to simply launch a block of size `128*128`, but that is not possible. We will use blocks of size `32*32`.\n",
        "\n",
        "We give you the code to launch the Kernel. You must compute the number of times a block must be launched horizontally (grid_rows) and vertically (grid_cols).\n",
        "\n",
        "```c\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(threadsPerBlock, threadsPerBlock);\n",
        "    MatMult<<<dimGrid, dimBlock>>>(gpu_A, gpu_B, gpu_C, N);\n",
        "```\n",
        "\n",
        "Then, use blockIdx.x, blockDim.x and threadIdx.x to compute the row inside the MatMult function, and blockIdx.y, blockDim.y and threadIdx.y to compute the column index.\n",
        "\n",
        "The goal of MatMult is to set `gpu_C[row * N + col]` to the sum of the product of the row of A and the col of B. Be sure to check for bounds !"
      ],
      "metadata": {
        "id": "c03agGMa_8Xq"
      }
    }
  ]
}